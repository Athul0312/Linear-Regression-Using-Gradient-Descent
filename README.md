# Linear Regression Using Gradient Descent

A from-scratch implementation of simple linear regression using manual gradient descent â€” no sklearn or built-in optimizers used.

---

## What I Did

- Loaded numeric (x, y) data and split into train/test sets
- Initialized slope and intercept
- Iteratively updated parameters using gradient descent
- Stopped when error dropped below a defined threshold

---

## Results

- Converged in ~43,000 steps
- Final learned model:    y = 0.96x + 3.16
- Visualized predictions vs actual data points on the test set

---

## Takeaway

Though small in scale, this project was crucial for building hands-on understanding of:
- Loss functions
- Gradients
- Model convergence

It laid the foundation for more advanced model tuning in later projects.
